{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import re\n",
    "from PIL import Image\n",
    "import pickle \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatar(path, maxsize):\n",
    "        img = Image.open(path).convert('L')   \n",
    "        WIDTH, HEIGHT = img.size\n",
    "        if WIDTH != HEIGHT:\n",
    "                m_min_d = min(WIDTH, HEIGHT)\n",
    "                img = img.crop((0, 0, m_min_d, m_min_d))\n",
    "        img.thumbnail(maxsize, Image.LANCZOS)\n",
    "        return np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar(path_dir, maxsize):\n",
    "        images = []\n",
    "        labels = []\n",
    "        os.chdir(path_dir)\n",
    "        for file in glob.glob(\"*.png\"):\n",
    "                img = formatar(file, maxsize)\n",
    "                if re.match('Leve.*', file):\n",
    "                        images.append(img)\n",
    "                        labels.append(0)\n",
    "\n",
    "                elif re.match('Carregado.*', file):\n",
    "                        images.append(img)\n",
    "                        labels.append(1)\n",
    "\n",
    "                elif re.match('sobrecarregado.*', file):\n",
    "                        images.append(img)\n",
    "                        labels.append(2)\n",
    "        return (np.asarray(images), np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsize = 250, 250\n",
    "def display_images(images, labels):\n",
    "        plt.figure(figsize=(80, 80))\n",
    "        grid_size = min(55, len(images))\n",
    "        for i in range(grid_size):\n",
    "                plt.subplot(11, 5, i+1)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.grid(False)\n",
    "                plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "                plt.xlabel(class_names[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels) = carregar(r'C:\\Users\\gusta\\OneDrive\\Documentos\\GitHub\\CargoSecurity\\treino', maxsize)\n",
    "(test_images, test_labels) = carregar(r'C:\\Users\\gusta\\OneDrive\\Documentos\\GitHub\\CargoSecurity\\teste', maxsize)\n",
    "\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(train_labels)))\n",
    "\n",
    "# Embaralhe os índices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use os índices embaralhados para embaralhar as duas listas\n",
    "train_images = train_images[indices]\n",
    "train_labels = train_labels[indices]\n",
    "\n",
    "class_names = [ 'leve', 'carregado', 'sobrecarregado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # 28x28\n",
    "model.add(Conv2D(32, (3,3), activation='relu',input_shape=(250,250,1) )) #20x24x24\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu')) #40x8x8\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu')) #40x8x8\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(192, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gusta\\OneDrive\\Documentos\\GitHub\\CargoSecurity\\Rede Neural.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/OneDrive/Documentos/GitHub/CargoSecurity/Rede%20Neural.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(\u001b[39m0.001\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/OneDrive/Documentos/GitHub/CargoSecurity/Rede%20Neural.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/OneDrive/Documentos/GitHub/CargoSecurity/Rede%20Neural.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gusta/OneDrive/Documentos/GitHub/CargoSecurity/Rede%20Neural.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m43\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:959\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    962\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    963\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[0;32m    964\u001b[0m           args, kwds))\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:418\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    409\u001b[0m         outputs \u001b[39m=\u001b[39m functional_ops\u001b[39m.\u001b[39mpartitioned_call(\n\u001b[0;32m    410\u001b[0m             args\u001b[39m=\u001b[39margs,\n\u001b[0;32m    411\u001b[0m             f\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m             config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    415\u001b[0m             executor_type\u001b[39m=\u001b[39mexecutor_type)\n\u001b[0;32m    417\u001b[0m \u001b[39mfor\u001b[39;00m i, func_graph_output \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph_outputs):\n\u001b[1;32m--> 418\u001b[0m   handle_data_util\u001b[39m.\u001b[39;49mcopy_handle_data(func_graph_output, outputs[i])\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m executing_eagerly:\n\u001b[0;32m    420\u001b[0m   \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\ops\\handle_data_util.py:41\u001b[0m, in \u001b[0;36mcopy_handle_data\u001b[1;34m(source_t, target_t)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy_handle_data\u001b[39m(source_t, target_t):\n\u001b[0;32m     27\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Copies HandleData for variant and resource type tensors if available.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m  The CppShapeInferenceResult::HandleData proto contains information about the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m    target_t: The tensor to copy HandleData to.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m   \u001b[39mif\u001b[39;00m (target_t\u001b[39m.\u001b[39;49mdtype \u001b[39m==\u001b[39;49m dtypes\u001b[39m.\u001b[39;49mresource \u001b[39mor\u001b[39;00m\n\u001b[0;32m     42\u001b[0m       target_t\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mvariant):\n\u001b[0;32m     43\u001b[0m     handle_data \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_handle_data(source_t)\n\u001b[0;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m (handle_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \u001b[39mand\u001b[39;00m handle_data\u001b[39m.\u001b[39mis_set\n\u001b[0;32m     46\u001b[0m         \u001b[39mand\u001b[39;00m handle_data\u001b[39m.\u001b[39mshape_and_type):\n",
      "File \u001b[1;32mc:\\Users\\gusta\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:245\u001b[0m, in \u001b[0;36mDType.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    242\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_type_enum \u001b[39m==\u001b[39m other\u001b[39m.\u001b[39;49m_type_enum\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=150, batch_size=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "my_dir\\intro_to_kt is not a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m tuner \u001b[39m=\u001b[39m kt\u001b[39m.\u001b[39mHyperband(model_builder,\n\u001b[0;32m     29\u001b[0m                      objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m                      max_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m     31\u001b[0m                      factor\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m     32\u001b[0m                      directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmy_dir\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     33\u001b[0m                      project_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mintro_to_kt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m stop_early \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(train_images, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[stop_early])\n\u001b[0;32m     39\u001b[0m \u001b[39m# Get the optimal hyperparameters\u001b[39;00m\n\u001b[0;32m     40\u001b[0m best_hps\u001b[39m=\u001b[39mtuner\u001b[39m.\u001b[39mget_best_hyperparameters(num_trials\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:220\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_create_trial()\n\u001b[1;32m--> 220\u001b[0m     trial \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moracle\u001b[39m.\u001b[39;49mcreate_trial(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtuner_id)\n\u001b[0;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m trial\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m trial_module\u001b[39m.\u001b[39mTrialStatus\u001b[39m.\u001b[39mSTOPPED:\n\u001b[0;32m    222\u001b[0m         \u001b[39m# Oracle triggered exit.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m         tf\u001b[39m.\u001b[39mget_logger()\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mOracle triggered exit\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     LOCKS[oracle]\u001b[39m.\u001b[39macquire()\n\u001b[0;32m    106\u001b[0m     THREADS[oracle] \u001b[39m=\u001b[39m thread_name\n\u001b[1;32m--> 107\u001b[0m ret_val \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m need_acquire:\n\u001b[0;32m    109\u001b[0m     THREADS[oracle] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:343\u001b[0m, in \u001b[0;36mOracle.create_trial\u001b[1;34m(self, tuner_id)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials[trial_id] \u001b[39m=\u001b[39m trial\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_order\u001b[39m.\u001b[39mappend(trial_id)\n\u001b[1;32m--> 343\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_trial(trial)\n\u001b[0;32m    344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[0;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m trial\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:640\u001b[0m, in \u001b[0;36mOracle._save_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_trial\u001b[39m(\u001b[39mself\u001b[39m, trial):\n\u001b[0;32m    638\u001b[0m     \u001b[39m# Write trial status to trial directory\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     trial_id \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39mtrial_id\n\u001b[1;32m--> 640\u001b[0m     trial\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_trial_dir(trial_id), \u001b[39m\"\u001b[39m\u001b[39mtrial.json\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_tuner\\engine\\oracle.py:634\u001b[0m, in \u001b[0;36mOracle._get_trial_dir\u001b[1;34m(self, trial_id)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_trial_dir\u001b[39m(\u001b[39mself\u001b[39m, trial_id):\n\u001b[0;32m    633\u001b[0m     dirname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_project_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrial_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(trial_id)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 634\u001b[0m     utils\u001b[39m.\u001b[39;49mcreate_directory(dirname)\n\u001b[0;32m    635\u001b[0m     \u001b[39mreturn\u001b[39;00m dirname\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras_tuner\\utils.py:46\u001b[0m, in \u001b[0;36mcreate_directory\u001b[1;34m(path, remove_existing)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_directory\u001b[39m(path, remove_existing\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     \u001b[39m# Create the directory if it doesn't exist.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(path):\n\u001b[1;32m---> 46\u001b[0m         tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mgfile\u001b[39m.\u001b[39;49mmakedirs(path)\n\u001b[0;32m     48\u001b[0m     \u001b[39m# If it does exist, and remove_existing is specified,\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[39m# the directory will be removed and recreated.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39melif\u001b[39;00m remove_existing:\n",
      "File \u001b[1;32mc:\\Users\\labin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:513\u001b[0m, in \u001b[0;36mrecursive_create_dir_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mio.gfile.makedirs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecursive_create_dir_v2\u001b[39m(path):\n\u001b[0;32m    503\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \n\u001b[0;32m    505\u001b[0m \u001b[39m  It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m   _pywrap_file_io\u001b[39m.\u001b[39;49mRecursivelyCreateDir(compat\u001b[39m.\u001b[39;49mpath_to_bytes(path))\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: my_dir\\intro_to_kt is not a directory"
     ]
    }
   ],
   "source": [
    "\n",
    "def model_builder(hp):\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model = Sequential() # 28x28\n",
    "  model.add(Conv2D(32, (3,3), activation='relu',input_shape=(250,250,1) )) #20x24x24\n",
    "  model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "  model.add(Conv2D(64, kernel_size=(3,3), activation='relu')) #40x8x8\n",
    "  model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "  model.add(Conv2D(128, kernel_size=(3,3), activation='relu')) #40x8x8\n",
    "  model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(units=hp_units, activation='relu'))\n",
    "  model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=100,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(train_images, train_labels, epochs=100, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step - loss: 11.3714 - accuracy: 0.4286\n",
      "42.85714328289032\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "acerto = str(test_acc*100)\n",
    "print(acerto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 117ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(test_images)\n",
    "result = []\n",
    "for x in a:\n",
    "    x = x.tolist()\n",
    "    result.append(x.index(max(x)))\n",
    "print(result)\n",
    "print(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregado (1).jpeg redimensionado com sucesso!\n",
      "Carregado (1).jpg redimensionado com sucesso!\n",
      "Carregado (1).png redimensionado com sucesso!\n",
      "Leve (1).jpeg redimensionado com sucesso!\n",
      "Leve (1).jpg redimensionado com sucesso!\n",
      "Leve (1).png redimensionado com sucesso!\n",
      "Leve (2).jpeg redimensionado com sucesso!\n",
      "Leve (2).jpg redimensionado com sucesso!\n",
      "Leve (2).png redimensionado com sucesso!\n",
      "Leve (3).jpeg redimensionado com sucesso!\n",
      "sobrecarregado005.png redimensionado com sucesso!\n",
      "sobrecarregado006.png redimensionado com sucesso!\n",
      "sobrecarregado007.png redimensionado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Pasta de entrada contendo as imagens originais\n",
    "pasta_de_entrada = r'C:\\Users\\labin\\OneDrive\\Documentos\\GitHub\\Constru-o-da-Rede-Neural\\teste'\n",
    "\n",
    "# Pasta de saída onde as imagens redimensionadas serão salvas\n",
    "pasta_de_saida = r'C:\\Users\\labin\\OneDrive\\Documentos\\GitHub\\Constru-o-da-Rede-Neural\\aa'\n",
    "\n",
    "# Tamanho desejado das imagens (300x300)\n",
    "novo_tamanho = (300, 300)\n",
    "\n",
    "# Certifique-se de que a pasta de saída exista ou crie-a\n",
    "if not os.path.exists(pasta_de_saida):\n",
    "    os.makedirs(pasta_de_saida)\n",
    "\n",
    "# Lista todos os arquivos na pasta de entrada\n",
    "arquivos = os.listdir(pasta_de_entrada)\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    try:\n",
    "        # Abre a imagem original\n",
    "        imagem = Image.open(os.path.join(pasta_de_entrada, arquivo))\n",
    "        \n",
    "        # Redimensiona a imagem para o tamanho desejado\n",
    "        imagem_redimensionada = imagem.resize(novo_tamanho)\n",
    "        \n",
    "        # Salva a imagem redimensionada na pasta de saída\n",
    "        imagem_redimensionada.save(os.path.join(pasta_de_saida, arquivo))\n",
    "        \n",
    "        print(f'{arquivo} redimensionado com sucesso!')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao redimensionar {arquivo}: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregado (1).jpeg convertido para PNG com sucesso!\n",
      "Carregado (1).jpg convertido para PNG com sucesso!\n",
      "Carregado (1).png convertido para PNG com sucesso!\n",
      "Leve (1).jpeg convertido para PNG com sucesso!\n",
      "Leve (1).jpg convertido para PNG com sucesso!\n",
      "Leve (1).png convertido para PNG com sucesso!\n",
      "Leve (2).jpeg convertido para PNG com sucesso!\n",
      "Leve (2).jpg convertido para PNG com sucesso!\n",
      "Leve (2).png convertido para PNG com sucesso!\n",
      "Leve (3).jpeg convertido para PNG com sucesso!\n",
      "sobrecarregado005.png convertido para PNG com sucesso!\n",
      "sobrecarregado006.png convertido para PNG com sucesso!\n",
      "sobrecarregado007.png convertido para PNG com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "pasta_de_entrada = r'C:\\Users\\labin\\OneDrive\\Documentos\\GitHub\\Constru-o-da-Rede-Neural\\teste'\n",
    "\n",
    "# Pasta de saída onde as imagens redimensionadas serão salvas\n",
    "pasta_de_saida = r'C:\\Users\\labin\\OneDrive\\Documentos\\GitHub\\Constru-o-da-Rede-Neural\\aa'\n",
    "\n",
    "# Certifique-se de que a pasta de saída exista ou crie-a\n",
    "if not os.path.exists(pasta_de_saida):\n",
    "    os.makedirs(pasta_de_saida)\n",
    "\n",
    "# Lista todos os arquivos na pasta de entrada\n",
    "arquivos = os.listdir(pasta_de_entrada)\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    try:\n",
    "        # Abre a imagem original\n",
    "        imagem = Image.open(os.path.join(pasta_de_entrada, arquivo))\n",
    "        \n",
    "        # Define o formato de saída como PNG\n",
    "        imagem.save(os.path.join(pasta_de_saida, f'{os.path.splitext(arquivo)[0]}.png'), 'PNG')\n",
    "        \n",
    "        print(f'{arquivo} convertido para PNG com sucesso!')\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao converter {arquivo} para PNG: {str(e)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
